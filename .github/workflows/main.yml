name: epglaozhang

on:
  schedule:
    - cron: "0 0 * * *"  # 每天午夜执行（可调整为其他时间）
  workflow_dispatch:     # 允许手动触发

jobs:
  scrape-data:
    runs-on: ubuntu-latest
    steps:
      # 1. 检出代码仓库
      - name: Checkout code
        uses: actions/checkout@v3

      # 2. 创建 Python 环境
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"  # 根据需要选择Python版本

      # 3. 安装依赖（如果需要）
      - name: Install dependencies
        run: |
          pip install -r requirements.txt  # 如果有依赖文件
          # 如果没有依赖项可以删除此步骤

      # 4. 执行默认抓取任务
      - name: Scrape default data
        run: python main.py
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}  # 从GitHub Secrets获取数据库连接信息
          API_KEY: ${{ secrets.API_KEY }}            # 如果需要其他API密钥

      # 5. 执行带-channel参数的抓取（抓取所有来源）
      - name: Scrape all channels
        run: python main.py -channel
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          API_KEY: ${{ secrets.API_KEY }}

      # 可选：添加错误处理（根据需要）
      - name: Check exit status
        if: ${{ failure() }}
        run: echo "任务执行失败，请检查日志"
